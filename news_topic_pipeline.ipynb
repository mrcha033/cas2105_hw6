{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini AI Pipeline: AG News Headline Classification\n",
    "\n",
    "This notebook follows the small-pipeline plan: baseline keyword rules vs. MiniLM embeddings + linear classifier. Run end-to-end (<10 minutes on CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install lightweight dependencies if needed. Comment out in managed environments that already include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets scikit-learn sentence-transformers torch pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and subsample data\n",
    "Using AG News, keep a tiny subset for speed: 2k train, 500 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "dataset = load_dataset('ag_news')\n",
    "label_names = dataset['train'].features['label'].names\n",
    "\n",
    "train_df = dataset['train'].to_pandas().sample(n=2000, random_state=SEED).reset_index(drop=True)\n",
    "test_df = dataset['test'].to_pandas().sample(n=500, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    return text.lower().strip()\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess)\n",
    "test_df['text'] = test_df['text'].apply(preprocess)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: keyword rules\n",
    "Simple keyword counts per class; ties broken by majority prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    'World': {'war', 'government', 'minister', 'president', 'iraq', 'election', 'peace'},\n",
    "    'Sports': {'win', 'wins', 'victory', 'coach', 'season', 'game', 'team', 'vs', 'cup'},\n",
    "    'Business': {'market', 'profit', 'shares', 'stocks', 'deal', 'company', 'dollar', 'trade'},\n",
    "    'Sci/Tech': {'software', 'research', 'technology', 'chip', 'internet', 'science', 'phone', 'data'}\n",
    "}\n",
    "label_to_name = dict(enumerate(label_names))\n",
    "name_to_label = {v: k for k, v in label_to_name.items()}\n",
    "majority_label = train_df['label'].mode().iloc[0]\n",
    "\n",
    "def baseline_predict(text: str) -> int:\n",
    "    tokens = text.split()\n",
    "    scores = {name: 0 for name in keywords}\n",
    "    for token in tokens:\n",
    "        for cls, vocab in keywords.items():\n",
    "            if token in vocab:\n",
    "                scores[cls] += 1\n",
    "    best_name = max(scores.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "    if scores[best_name] == 0:\n",
    "        return majority_label\n",
    "    return name_to_label[best_name]\n",
    "\n",
    "baseline_preds = [baseline_predict(t) for t in test_df['text']]\n",
    "baseline_acc = accuracy_score(test_df['label'], baseline_preds)\n",
    "baseline_f1 = f1_score(test_df['label'], baseline_preds, average='macro')\n",
    "print(f'Baseline accuracy: {baseline_acc:.3f}, macro-F1: {baseline_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI pipeline: MiniLM embeddings + logistic regression\n",
    "Embed headlines, then train a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "train_embeddings = encoder.encode(train_df['text'].tolist(), batch_size=64, show_progress_bar=True, convert_to_numpy=True, device='cuda' if encoder.device.type == 'cuda' else None)\n",
    "test_embeddings = encoder.encode(test_df['text'].tolist(), batch_size=64, show_progress_bar=True, convert_to_numpy=True, device='cuda' if encoder.device.type == 'cuda' else None)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, C=4.0, multi_class='multinomial', n_jobs=-1)\n",
    "clf.fit(train_embeddings, train_df['label'])\n",
    "\n",
    "preds = clf.predict(test_embeddings)\n",
    "acc = accuracy_score(test_df['label'], preds)\n",
    "macro_f1 = f1_score(test_df['label'], preds, average='macro')\n",
    "print(f'Pipeline accuracy: {acc:.3f}, macro-F1: {macro_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative differences\n",
    "Collect examples where baseline and pipeline disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for text, true_label, base_pred, model_pred in zip(test_df['text'], test_df['label'], baseline_preds, preds):\n",
    "    if base_pred != model_pred:\n",
    "        diffs.append({\n",
    "            'text': text,\n",
    "            'true': label_to_name[true_label],\n",
    "            'baseline': label_to_name[base_pred],\n",
    "            'pipeline': label_to_name[model_pred],\n",
    "        })\n",
    "\n",
    "pd.DataFrame(diffs).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report (optional detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_df['label'], preds, target_names=label_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
